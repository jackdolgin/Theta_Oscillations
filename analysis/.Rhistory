fluidRow(class = "text-center",
column(4, h3("Quantification and Statistical Analyses"), offset = 3)), br(), br(),
fluidRow(column(4, br(),
switchInput("iso_sides", "Separate Hemifields", labelWidth = 150), br(),
switchInput("sbtr", "Analyze Invalid - Valid", labelWidth = 150), helpText("Subtract the dependent variable values at each CTI (valid - invalid) before performing analyses rather than analyzing valid and invalid trials independently"), br(),
numericInput("samp_per", "Sampling Period", min = round(1 / 60, 4), max = round(30 / 60, 4), value = round(1 / 60, 4), step = round(1 / 60, 4)), helpText(paste0("Spacing between CTI intevals (in seconds); the data was originally sampled at ", round(1 / 60, 4), ", but one could re-sample at a different rate, which would just clump neighboring CTI's together (whereas the below field groups neighbors but doesn't combine them, maintaining the total number of bins)")), br(),
numericInput("clumps", "Neighbors to average at each CTI", min = 0, max = 14, value = 2, step = 2), helpText("`0` means this function does nothing, `2` means each CTI is the average of that CTI and its neighboring CTI's on each sides, etc...")
),
column(3,
radioGroupButtons("dep_var", "Dependent Variable", c("Accuracy", "Response Time"), selected = "Accuracy", status = "primary"), br(),
numericInput("pval", "P-value", max = .99, value = .05), helpText("The p-value to use for drawing the significance cutoff on the graphs"), br(),
numericInput("shuff", "Surrogate Shuffles for Null Hypothesis", min = 1, max = 10000, value = 50, step = 1), helpText("NOTE: increasing this number slows down the run time")
),
column(5, br(),
checkboxGroupButtons("trends", choices = c("Detrending", "Demeaning"), selected = c("Detrending", "Demeaning"), status = "primary"), br(),
radioGroupButtons("smooth_method", "Smoothing Individuals\" Time-Series", c("GLM", "GAM", "Loess", "LM"), selected = "Loess", status = "primary"), br(),
radioGroupButtons("win_func", "Windowing Function", c("Cosine", "Hamming", "Hann", "Kaiser", "Square", "Triangle", "Tukey", "Welch"), selected = "Tukey", status = "primary"), br(),
numericInput("xaxisvals", "Max Hz Displayed", min = 1, value = 15), br(),
numericInput("duration", "Duration (Seconds) Analyzed Including Padding", min = .8, max = 10, value = 1, step = .001)
)),
fluidRow(class = "text-center",
column(4, br(), br(), h3( "Filtering Participants"), offset = 3)), br(), br(),
fluidRow(column(4, br(),
switchInput("attn_filter", "Attention Filter", labelWidth = 100), helpText("Remove participants who reported either dozing off or not fully concentrating on multiple blocks"), br(),
numericInput("catch_floor", "Catch-Trial Accuracy Floor", min = 0, max = 1, value = .85), helpText("Filter out participants whose filtered data is outside of the selected range")
),
column(4, br(),
numericInput("side_bias", "Side Bias Ceiling", min = .01, max = .99, value = .2), helpText("Filter out participants whose hit rate at one visual field - another visual field is > this ceiling"), br(),
numericInput("wm_floor", "Working Memory Accuracy Floor", min = .01, max = .99, value = .7), helpText("Filter out participants whose working memory task accuracy is < this cutoff"), br(),
numericInput("invalid_floor", "Invalid Trial Accuracy Floor", min = .01, max = .99, value = .15), helpText("Filter out participants whose accuracy on invalid trials is < this cutoff")
),
column(4, br(),
sliderInput("pre_range", "Pre-Filtered Accuracy Cutoffs", min = 0, max = 1, value = c(.45, .85)), helpText("Remove participants whose unfiltered accuracy is outside this range"), br(),
sliderInput("post_range", "Post-Filtered Accuracy Cutoffs", min = 0, max = 1, value = c(.45, .85)), helpText("Remove participants whose filtered data is outside of the selected range")
)),
fluidRow(class = "text-center", br(),
column(4, h3( "Filtering Trials"), offset = 3)), br(), br(),
fluidRow(column(4, br(),
sliderInput("block_range", "Block Accuracy Range", min = 0, max = 1, value = c(.20, .80)), helpText("Interpolate over trials if the average hit rate in that block, every 48 trials, was below this value"), br(),
checkboxGroupButtons("blocks_desired", "Blocks Included in Analysis", choices = 1:8, selected = 1:8, status = "primary")),
column(4,
sliderInput("miniblock_range", "Mini-Block Accuracy Cutoffs", min = 0, max = 1, value = c(.40, .80)), helpText("Interpolate over trials if the average hit rate in that mini-block, every 16 trials which is how often the task difficulty was adjusted to titrate to 65%, is outside this range")),
column(4, br(),
sliderInput("CTI_range", "Remove CTI\'s Outside This Range", min = .3, max = 1.29, value = c(.3, 1.09)))), br(), br())
server <- function(input, output, session) {
grouping_cnsts <- quos(participant, Trials_filtered_out, Acc_prefilter, Acc_postfilter, CatchAcc)
observe({
pcpts <- if (input$dset == "Pilot") {
blocksize <- 54
if(input$ext_objects == "2-object Task") 301:324 else 401:427
} else {
blocksize <- 80
if (input$wm_exp){
701:730
} else {
if(input$ext_objects == "2-object Task") c(501:522, 524:534) else c(601:631)}
}
dep_var_abbr <- as.name(ifelse(input$dep_var == "Accuracy", "Acc", "RT"))
dem_df <- fread(file.path("data", "Demographics.csv")) %>%
mutate_at(vars(SubjID), as.numeric)
# For each participant, function reads in data, filters it, and transforms it to prepare for interpolation and FFT'ing
pcpts_combine <- function(pcpt){
fread(file.path("data", pcpt, paste0(pcpt, ".csv"))) %>%# Reads in participant data (the `select` part is because PsychoPy created an empty column at the end of the data frame for the first few participants, which meant those dataframes had different dimensions than subsequent data frames, which `do.call` doesn't like)
filter(Trial > 0) %>%                                                   # Prunes practice trials
mutate(Stim_Sides = as.character(
ifelse(CorrSide == FlashSide, "Valid", "Invalid")),            # Creates column indicating whether cue was valid or invalid
CTI = RoundTo(RoundTo(lilsquareStartTime - flash_circleEndTime,
1 / 60), input$samp_per),
block = RoundTo(Trial, blocksize, ceiling) / blocksize,          # Creates column indicating trial's block
RT = ifelse(Acc == 1 & ButtonPressTime - lilsquareStartTime > .1,#                           RT after target appeared on screen, only for correct trials with an RT > 100 ms
ButtonPressTime - lilsquareStartTime, NA)) %>%
mutate_at(vars(Acc, RT), list(~ifelse(block %in% input$blocks_desired,  # Converts trials' Acc and RT to NA if they are not in a desired block
., NA))) %>%
mutate(wm_Acc = ifelse(session == "4",                                  # Creates column indicating wm accuracy; na.rm = TRUE because majority of trials lack wm probe, create NA
mean(Acc_wmarith, na.rm = TRUE), 1),
CatchAcc = mean(ifelse(Opacity != 0, NA, Acc), na.rm = TRUE)) %>%#                           mean accuracy for catch trials
filter(CatchAcc >= input$catch_floor,                                   # Prunes participants whose catch accuracy is below desired threshold
Opacity != 0) %>%                                                #        catch trials
filter(mean(Acc[Stim_Sides == "Invalid"],                                 #        participants whose invalid trial accuracy is...
na.rm = TRUE) >= input$invalid_floor) %>%                           #        ...below desired threshold
mutate(CorrSide = case_when(CorrSide == 1 ~ "Right",                    #                           which side the target appeared on
CorrSide == -1 ~ "Left", TRUE ~ "Bottom"),
Stim_Sides = case_when(input$iso_sides ~ paste(CorrSide, Stim_Sides,  # Overwrites `Stim_Sides` column if `sep_vis_fidels` parameter == `TRUE`
sep = "_"),           # to include which side of screen target was on,
TRUE ~ Stim_Sides)) %>%                       # as well as whether it was valid with cue; if `iso_sides` parameter == `FALSE`, leaves `StimSides`
# unchanged
mutate(Acc_wmarith = ifelse(session == "4", Acc_wmarith, NA)) %>%
#        Acc = ifelse(Acc_wmarith %in% 0, NA, Acc)) %>%
group_by(CorrSide) %>%
mutate(Side_Acc = mean(Acc, na.rm = TRUE)) %>%
ungroup() %>%
mutate(Side_Diff = max(Side_Acc) - min(Side_Acc)) %>%
left_join(dem_df, by = c("participant" = "SubjID")) %>%
filter(grepl(ifelse(input$attn_filter, "task" , ""), Attentiveness),    # Prunes participants reporting lack of alertness on at least two blocks, when `attn_filter` == TRUE
Side_Diff <= input$side_bias,                                    #                     whose hit rate at one visual field - another visual field is > `side_bias`
wm_Acc >= input$wm_floor) %>%                                    #                           working memory task accuracy is < `wm_floor`
mutate(Acc_prefilter = mean(Acc, na.rm = TRUE)) %>%                     # Creates column indicating mean accuracy before we've filtered for `block_range`, unlike `Acc_postfilter`
filter(between(Acc_prefilter, input$pre_range[1], input$pre_range[2],   # Prunes participants whose non-catch, pre-block-filtering accuracy is outside of desired range
incbounds = TRUE),
between(CTI, min(input$CTI_range), max(input$CTI_range))) %>%    #        trials outside of desired CTI range
group_by(block) %>%
mutate(block_acc = mean(Acc)) %>%                                       # Creates column indicating block's mean accuracy
ungroup() %>%
mutate(rown = row_number(),
miniblock = RoundTo(rown, 16, ceiling) / 16) %>%                 #                           trial's mini-block (every 16 non-catch trials the opacity was readjusted)
group_by(miniblock) %>%
mutate(miniblock_avg = mean(Acc)) %>%
ungroup() %>%
mutate_at(vars(Acc, RT),
list(~ifelse(between(block_acc, input$block_range[1],         # Changes `Acc` and `RT` column values to NA if trial's
input$block_range[2]) &                  # block accuracy outside of `block_range`
between(miniblock_avg, input$miniblock_range[1], # or miniblock was not in the desired range
input$miniblock_range[2]), ., NA))) %>% # or block was not in `blocks_desired`
mutate_at(vars(Acc, RT), list(~ifelse(session != "4"  | Acc_wmarith == 1, ., NA))) %>%
mutate(Trials_filtered_out = sum(is.na(Acc)) / n(),
Acc_postfilter = mean(Acc, na.rm = TRUE)) %>%                    # Creates column indicating mean accuracy for non-catch trials; note this is after before we've filtered
# for `block_range`, unlike `Acc_prefilter`
filter(between(Acc_postfilter, input$post_range[1], input$post_range[2])) %>% # Prunes participants whose non-catch, post-block-filtering accuracy is outside of desired range
group_by(CTI, Stim_Sides, !!!grouping_cnsts) %>%
summarise_at(vars(Acc, RT), list(~mean(., na.rm = TRUE))) %>%           # Overwrites `Acc` and `RT` columns according to mean of each combination of `CTI` and `Stim_Sides`
arrange(CTI) %>%
group_by(Stim_Sides) %>%
mutate_at(vars(Acc, RT), list(~na.approx(., na.rm = FALSE, rule = 2))) %>% # If any combination of `CTI` and `Stim_Sides` has only NA values, it takes on the average of its
# neighboring CTI with same `Stim_Sides`
mutate_at(vars(Acc, RT), list(~rollapply(., input$clumps + 1, mean,     # Averages each CTI with neighbors
partial = TRUE)))
}
cmbd <- do.call(rbind, lapply(pcpts, pcpts_combine)) %>%                    # Calls `pcpts_combine` function for argumenet `pcpts`; then combines each participant's dataframe into one
arrange(Acc_prefilter, participant, Stim_Sides, CTI)
CTIs <- unique(cmbd$CTI)
if (input$win_func == "Tukey"){ win <- tukeywindow(length(CTIs), .5)} else { # Creates window, which if `tukey` will add the parameter `r` == `.5` —so 'only' half the data length will be non-flat
win <- match.fun(paste0(tolower(input$win_func), "window"))(length(CTIs))}
locations <- unique(cmbd$Stim_Sides)                                   # Creates vector of column names representing sides locations of target in reference to cue (and also potentially side of screen)
pcpts <- unique(cmbd$participant)                                       # Creates vector of remaining participant numbers after `pcpts_combine` filtering
cmbd_w <- cmbd %>%
pivot_wider(CTI:CatchAcc, Stim_Sides, values_from = !!dep_var_abbr) %>%
arrange(Acc_prefilter, participant, CTI) %>%
group_by(participant) %>%
mutate_at(vars(locations), list(~na.approx(., na.rm = FALSE, rule = 2)))
# Analyzes Invalid - Valid instead of them separetely, if sbtr == `Yes`
if (input$sbtr){
s <- tail(1:ncol(cmbd_w), length(locations)) [c(TRUE, FALSE)]
cmbd_w[paste0(names(cmbd_w[s]), "_minus_",
names(cmbd_w)[s + 1])] <- cmbd_w[s] - cmbd_w[s + 1]
locations = tail(colnames(cmbd_w), length(locations) / 2)
}
# Determines confidence intervals
conf_int <- function(x, ...){ x %>%
group_by(.dots = lazy_dots(...)) %>%
summarise_at(vars(locations), list(~qnorm(.975) * std_err(.)))
}
# Transforms from Time to Frequency Domain
amplitude <- function(x, y){
pre_pad <- nrow(x)                                                          # Number of rows expected with one row per pcpt per CTI per shuffle
x %>%
ungroup() %>%
mutate(samp_shuff = RoundTo(row_number(), pre_pad / y,                    # Creates variable to track shuffle number which is then used for group_by...
ceiling) / (pre_pad / y)) %>%
group_by(participant, samp_shuff) %>%
mutate_at(vars(locations),
list(~ case_when("Detrending" %in% input$trends ~                     # If `Detrending` selected...
. - polyval(polyfit(CTI, ., 2), CTI),        # detrend with this formula...
TRUE ~ .))) %>%                                # otherwise ignore
mutate_at(vars(locations), list(~case_when("Demeaning" %in% input$trends ~      # Works just like the detrending except for demeaning
. - mean(.), TRUE ~ .))) %>%
mutate_at(vars(locations), list(~ . * win / Norm(win))) %>%               # Apply window
ungroup() %>%
add_row(participant = rep(pcpts,                                          # Add empty rows (other than participant name) as additional CTI's needed to reach desired padded...
y * (floor((input$duration - diff(range(              # `duration` of intervals for each participant for...
cmbd_w$CTI))) / input$samp_per)))) %>%              # each shuffle (`y` represents each shuffle)
mutate_at(vars(locations), list(~coalesce(., 0))) %>%                     # Add zeros in newly-created empty rows for locations columns to zero-pad them; note these rows follow...
# non-padded data, i.e. they are tailing zeros that are padding on the back-end
mutate_at(vars(samp_shuff), list(~coalesce(., RoundTo(                    # Tags the padded rows with one of the shuffles created earlier with `samp_shuff`...
row_number() - pre_pad,                                                 # the non-padded rows (<= pre_pad) appear first and have already been tagged, so we keep them as they are
(n() - pre_pad) / (y),
ceiling) / ((n() - pre_pad) / y)))) %>%
group_by(participant, samp_shuff) %>%                                     # This group_by is critical so we're only taking the FFT of each shuffle
mutate_at(vars(locations), list(~Mod(sqrt(2 / n()) * fft(.)) ^ 2)) %>%    # Tabulate amplitude
mutate(Hz = (row_number() - 1) / (n() * input$samp_per)) %>%                    # Set Hz corresponding to each amplitude
ungroup() %>%
select(-CTI)
}
amps <- amplitude(cmbd_w, 1)
fft_x <- round(1 / (length(unique(amps$Hz)) * input$samp_per),1)
xaxis_r <- RoundTo(input$xaxisvals, fft_x)
# Set Up Graphing
t_srs_g <- function(x){
cmbd_w %>%
gather(Location, !!dep_var_abbr, -c(CTI, !!!grouping_cnsts)) %>%
right_join(gather(conf_int(cmbd_w, CTI), Location, Conf_Int, -CTI),
by = c("CTI", "Location")) %>%
group_by(CTI, Location, Conf_Int, !!!head(grouping_cnsts, x)) %>%
summarise(!!dep_var_abbr := mean(!!dep_var_abbr)) %>%                               # Keeps either `RT` or `Acc` column—depending on whether `dep_var_abbr` parameter == `RT` or `Acc`
ggplot(aes(CTI, !!dep_var_abbr, group = Location, color = Location,
fill = Location, ymin = !!dep_var_abbr - Conf_Int,
ymax = !!dep_var_abbr + Conf_Int)) +
labs(title = paste0(input$dep_var, " by Cue-Target Interval ", input$ext_objects),
x = "Cue-Target Interval (ms)")
}
fft_g <- function(x) { x +
labs(title = paste0("FFT of Target ", input$dep_var, ", ", input$ext_objects),
col = "Target Location", y = "Spectral Power") +
theme(panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank())
}
viridis_cols <- .7 + RoundTo(.0001 * RoundTo(length(locations),
4, floor), .2, ceiling)
graph <- function(y, x) {
y(x) +
theme_bw() +
scale_color_viridis_d(option = "C",
end = viridis_cols,
labels = sapply(locations, simplify = TRUE, function(x) gsub("_", " ", x),
USE.NAMES = FALSE)) +
scale_fill_viridis_d(option = "C",
end = viridis_cols) +
guides(colour = guide_legend(reverse = TRUE), fill = FALSE)
}
idvl_g <- function(y, x){
graph(y, x) +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5)) +
facet_wrap(~factor(participant, levels = pcpts),
ncol = RoundTo(length(pcpts) / 4, 1, ceiling),  scales = "free_x")                                   # `free` means the y_axis isn't fixed from participant to participant
}
cmbd_g <- function(y, x) {
graph(y, x) + geom_line(size = 1.5) +
theme(legend.key.size = unit(.55, "in")) +
labs(subtitle = paste( "Data from", as.character(length(pcpts)),
"participants"))
}
if (input$display == "Time-Series + FFT by Individual"){
# Produces left half of final graph
ts_facets <- idvl_g(t_srs_g, 1) +
geom_line(alpha = I(2 / 10), color = "grey", show.legend = FALSE) +       # Graphs unsmoothed data in light gray
stat_smooth(method = tolower(input$smooth_method), span = 0.2, se = FALSE,               # Smoothes data depending on `smooth_method` parameter
size = .5, show.legend = FALSE)
# Produces label for each right-side graph
plot_label <- amps %>%
group_by(!!!grouping_cnsts) %>%
summarise() %>%
ungroup() %>%
drop_na() %>%
mutate_at(vars(!!!tail(grouping_cnsts, -1)),
funs(paste(quo_name(quo(.)), "=", percent(.)))) %>%
unite(lab, !!!tail(grouping_cnsts, -1), sep = "\n", remove = FALSE)
# Produces right half of final graph
fft_facets <- idvl_g(fft_g, amps %>%
group_by(participant, Hz) %>%
summarise_all(mean) %>%
gather(Flash_and_or_field, Power, -Hz, -samp_shuff, -c(!!!grouping_cnsts)) %>%
ggplot(aes(Hz, Power, color = Flash_and_or_field))) +
geom_line() +
scale_x_continuous(name = "Frequency (Hz)", limits = c(0, xaxis_r),
breaks = seq(0, input$xaxisvals, ifelse(input$xaxisvals > 10 | input$xaxisvals != xaxis_r, 1/max(Closest(xaxis_r/ seq(fft_x, xaxis_r, fft_x), 5) /xaxis_r), fft_x))) +
labs(caption = paste("Data from", as.character(length(pcpts)),
"participants")) +
geom_text(data = as.data.frame(plot_label), inherit.aes = FALSE, size = 2.5,# Sets location for label overlayed onto graph
aes(label = lab, x = Inf, y = Inf), vjust = 1.15, hjust = 1.05)
output$mygraph <- renderPlot({ grid.arrange(ts_facets, fft_facets, ncol = 2)
}, height = 800, width = 1350)                # Combines time series and FFT graphs into one plot
}  else if (input$display == "Time-Series Across Participants") {
output$mygraph <- renderPlot({
move_layers(cmbd_g(t_srs_g, 0) +
theme(panel.grid = element_blank()) +
geom_ribbon(alpha = 0.15, aes(color = NULL)), "GeomRibbon", position = "bottom")
}, height = 800, width = 1350)
} else { # Graph combined FFT ------------------------------------------------
set.seed(123)
# Produces `shuff` # of null hypothesis permutations
shuffle <- function(x){
cmbd_w %>%
group_by(participant) %>%
sample_n(length(CTIs), weight = CTI) %>%
mutate_at(vars(CTI), list(~round(seq(min(CTIs), max(CTIs), input$samp_per), 2)))
}
# Produces and save graph
amps_shuff <- do.call(rbind, mclapply(1:input$shuff, shuffle)) %>%
amplitude(input$shuff) %>%
group_by(Hz, samp_shuff) %>%
summarise_at(vars(locations), mean) %>%
group_by(Hz) %>%
summarise_at(vars(locations), list(~quantile(., probs = 1 - input$pval))) %>%
combine(amps %>% group_by(Hz) %>% summarise_at(vars(locations), mean),
names = (c("Significance Cutoff", "Observed Data"))) %>%
gather(Location, Power, -c(Hz, source)) %>%
right_join(gather(conf_int(amps, Hz), Location, Conf_Int, -Hz),
by = c("Hz", "Location"))
output$mygraph <- renderPlot({
(move_layers(cmbd_g(fft_g, ggplot(amps_shuff, aes(Hz, Power, col = Location, linetype = source,
ymin = Power - Conf_Int, ymax = Power + Conf_Int, fill = Location))) +
scale_linetype_manual(values = c("solid", "dashed")) +
scale_x_continuous(name = "Frequency (Hz)", limits = c(0, input$xaxisvals),
breaks = seq(0, input$xaxisvals, ifelse(fft_x > .5, round(fft_x, 2), 1))) +
labs(linetype = "",
caption = paste("Significance threshold at p < ",
as.character(input$pval))) +
geom_ribbon(data = filter(amps_shuff, source == "Observed Data"), alpha = 0.15, aes(color = NULL)) +
geom_point(size = 3, data = amps_shuff %>% spread(source, Power) %>%
filter(`Observed Data` > `Significance Cutoff`) %>%
select(-c(`Significance Cutoff`, Conf_Int)) %>%
gather(source, Power, -Hz, -Location),
aes(ymin = NULL, ymax = NULL)), "GeomRibbon", position = "bottom"))
}, height = 800, width = 1350)
}
})
}
shinyApp(ui, server)
---
geometry: "left=.3cm,right=4.6cm,top=3cm,bottom=4.5cm"
output:
pdf_document:
latex_engine: xelatex
extra_dependencies: ["xcolor", "hyperref", "fontspec", "array", "setspace", "longtable"]
---
\setmainfont[Color={444444}]{Humanist 521 Light BT}
\makeatletter
\def\Hy@href#{\addfontfeatures{Color=047464}\hyper@normalise\href@}
\newfontfamily\MyRed[Color={a41414}]{Gill Sans Nova Cn Bold}
\def\sectitle{\MyRed \fontsize{12}{10} \selectfont}
\newfontfamily\MyName [Color={444444}]{Adobe Garamond Pro Bold}
\def\jmp{[.5cm]}
\def\ind{\hspace{.37cm} \hangindent=.45cm }
\def\HeadSpace{.15\textwidth}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedleft\arraybackslash}p{.25\textwidth}}
& \begingroup\MyName \fontsize{20}{10} \selectfont Jack Dolgin \endgroup  &
\\
& Center for Cognitive Neuroscience                                       & \href{https://twitter.com/jbdolg}{@jbdolg}
\\
& LSRC Box 90999                                                          & \href{mailto:jack.dolgin@duke.edu}{jack.dolgin@duke.edu}
\\
& Duke University, Durham, NC 27708                                       & \href{<jackdolgin.org}{jackdolgin.org}
\end{longtable}
\setmainfont[Color={444444}]{Adobe Garamond Pro}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedleft\arraybackslash}p{.25\textwidth}}
\begingroup\sectitle EDUCATION\endgroup  & Duke University                                                                                   & Durham, NC
\\
& \ind B.S., Psychology, with Distinction                                                           &
\\
& \ind Thesis: \emph{Separating the Influence of Budget and Numeric Priming on Willingness to Pay}  &
\end{longtable}
\renewcommand{\arraystretch}{1.5}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedleft\arraybackslash}p{.25\textwidth}}
\begingroup\sectitle EXPERIENCE\endgroup  & Duke University                                                               & Durham, NC
\\
& \ind Lab Manager, Egner Lab                                                   & June 2018 -
\\
& \ind Research Assistant, Huettel Lab                                          & Jan. 2017 - May 2018
\\
& \ind Research Assistant, Cognitive-Behavioral Research and Treatment Program  & May 2016 - Dec. 2016
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.05\textwidth} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedright\arraybackslash}p{0.15\textwidth}}
\begingroup\sectitle GRANTS\endgroup  & 2019  & Interdisciplinary Behavioral Research Center Mini-Grant from Duke University (\$600)  &
\\
& 2018  & Faculty Seed Grant from Duke University, with Peter S. Whitehead and Tobias Egner (\$33,620)  &
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.05\textwidth} >{\raggedright\arraybackslash}p{0.3\textwidth} >{\raggedright\arraybackslash}p{0.45\textwidth}}
\begingroup\sectitle AWARDS\endgroup  & 2017  & \href{https://psychandneuro.duke.edu/summer-vertical-integration-program-vip}{Vertical Integration Program} \newline Duke University (\$3,500) & Selected to be funded for a summer of research at Duke University before my senior year
\\
& 2016  & \href{https://library.duke.edu/research/awards/aptman}{Lowell Aptman Prize} \newline Duke University (\$1,000)                                & Awarded the best research \href{https://hdl.handle.net/10161/16739}{paper} in any subject among all freshmen and sophomores at Duke University
\\
& 2015  & \href{https://library.duke.edu/research/awards/holsti}{Ole R. Holsti Prize} \newline Duke University (\$1,000) & Awarded, for a \href{http://hdl.handle.net/10161/10217}{separate paper}, the best research paper in political science using primary sources among all freshmen and sophomores at Duke University
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.8\textwidth}}
\begingroup\sectitle MANUSCRIPTS\endgroup  & \textbf{In Progress}                                                                                                                                                                  \\
& Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. Individual differences in the use of variable budget information in consumer choice. Submitted.                               \\
& Bejjani, C., \textbf{Dolgin, J.}, Zhang, Z., and Egner, T. (\href{https://osf.io/7jfbp/}{Preregistered Direct Replication, Stage 1 in principle acceptance at \emph{Psychological Science}}). Disentangling the roles of cue visibility and preparatory knowledge in learning cognitive control.
\\
& \textbf{Dolgin, J.}, Fiebelkorn, I.C., and Egner, T. TBD. In preparation.
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.8\textwidth}}
{\begingroup\sectitle TALKS \& PRESENTATIONS\endgroup}  & \textbf{Refereed} \begin{spacing}{1.5} \par \end{spacing} Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. (2019, June). Individual differences in the use of variable budget information in consumer choice. Talk delivered at the 9th annual Interdisciplinary Symposium on Neuroscience, Durham, N.C.
\\
& Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. (2019, May). \href{content/posters/Amasino_et_al_2019_APS.pdf}{Individual differences in the use of variable budget information in consumer choice}. Poster presented at the 31st annual meeting of the Association for Psychological Science, Washington, D.C.
\\
& \textbf{Dolgin, J.}, Bejjani, C., Zhang, Z., and Egner, T. (2019, May). \href{content/posters/Dolgin_et_al_2019_APS.pdf}{Disentangling the roles of cue visibility and knowledge in learning cognitive control}. Poster presented at the 31st annual meeting of the Association for Psychological Science, Washington, D.C.
\\
& Zhang, Z., Bejjani, C., \textbf{Dolgin, J.}, and Egner, T. (2019, Mar.). \href{content/posters/Zhang_et_al_2019_CNS.pdf}{Disentangling the roles of cue visibility and knowledge in learning cognitive control}. Poster presented at the 26th annual meeting of the Cognitive Neuroscience Society, San Francisco, CA.
\\
& Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. (2018, Oct.). \href{content/posters/Amasino_et_al_2018_SNE.pdf}{Individual differences in the use of variable budget information in consumer choice}. Poster presented at the 16th annual meeting of the Society for NeuroEconomics, Philadelphia, PA.
\\
& \textbf{Non-Refereed}
\\
& Zhang, Z., Bejjani, C., Chiu, Y.C., \textbf{Dolgin, J.}, and Egner, T. (2019, July). Neural evidence of control state reinstatement: an fMRI study. Poster presented at the Duke Summer Undergraduate Research Showcase, Durham, NC.
\\
& \textbf{Dolgin, J.} (2019, May). NCAA women’s basketball as a proxy for gender differences in verbal aggression. Talk delivered at the Duke Center for Cognitive Neuroscience Data Blitz, Chapel Hill, NC.
\\
& \textbf{Dolgin, J.}, Amasino, D., and Huettel, S.A. (2018, Apr.). \href{content/posters/Dolgin_et_al_2018_Visible_Thinking.pdf}{Separating the influence of budget and numeric priming on willingness to pay}. Poster presented at the Duke Visible Thinking Poster Fair, Durham, NC.
\\
& \textbf{Dolgin, J.}, Amasino, D., and Huettel, S.A. (2017, July). \href{content/posters/Dolgin_et_al_2017_VIP.pdf}{Budget’s Effect on consumers’ willingness-to-pay}. Poster presented at the Duke Vertical Integration Program Poster Fair, Durham, NC.
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.35\textwidth} >{\raggedright\arraybackslash}p{0.45\textwidth}}
\begingroup\sectitle SIDE PROJECTS\endgroup  & \textbf{Dolgin, J.}, Whitehead, P.S., Vieth, A.Z. (In Preparation). NCAA women’s basketball reveals men as more verbally aggressive. & Foray into 50,000 basketball games with web-scraping—finishing a project idea I had to propose for Dr. Vieth’s class as an undergraduate—to complete the first observational study in adults to date on differences in verbal aggression between genders.
\\
& Butchireddygari, L., \textbf{Dolgin, J.} (2018). \href{https://www.dukechronicle.com/article/2018/01/is-greek-life-at-duke-as-homogenous-as-you-think}{Is Greek life at Duke as homogeneous as you think?} Duke Chronicle.                  & My coauthor and I collected over 30 variables on all 1,700 students in Duke’s class of 2018 to analyze trends in the demographics of members of Greek life, and we posted all our publicly-collected data to a repository and I analyzed it in an interactive \href{https://jackdolgin.shinyapps.io/GreekLifeDemographics/}{Shiny app}.
\\
& \textbf{Dolgin, J.} (2017). \href{https://www.dukechronicle.com/search?a&ti=history&au=Dolgin}{This Week in Duke Sports History}. Duke Chronicle.                                                        & My work documenting a zany fact in Duke sports history for an entire year was a finalist in Duke’s annual award for a journalism project or article across all of Duke’s undergraduates, including journalism certificate students and the rest of the school.
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.35\textwidth} >{\raggedright\arraybackslash}p{0.45\textwidth}}
\begingroup\sectitle SKILLS\endgroup  & \textbf{Dolgin, J.}, Whitehead, P.S., Vieth, A.Z. (In Preparation). NCAA women’s basketball reveals men as more verbally aggressive. & Foray into 50,000 basketball games with web-scraping—finishing a project idea I had to propose for Dr. Vieth’s class as an undergraduate—to complete the first observational study in adults to date on differences in verbal aggression between genders.
\end{longtable}
---
geometry: "left=.3cm,right=4.6cm,top=3cm,bottom=4.5cm"
output:
pdf_document:
latex_engine: xelatex
extra_dependencies: ["xcolor", "hyperref", "fontspec", "array", "setspace", "longtable"]
---
\setmainfont[Color={444444}]{Humanist 521 Light BT}
\makeatletter
\def\Hy@href#{\addfontfeatures{Color=047464}\hyper@normalise\href@}
\newfontfamily\MyRed[Color={a41414}]{Gill Sans Nova Cn Bold}
\def\sectitle{\MyRed \fontsize{12}{10} \selectfont}
\newfontfamily\MyName [Color={444444}]{Adobe Garamond Pro Bold}
\def\jmp{[.5cm]}
\def\ind{\hspace{.37cm} \hangindent=.45cm }
\def\HeadSpace{.15\textwidth}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedleft\arraybackslash}p{.25\textwidth}}
& \begingroup\MyName \fontsize{20}{10} \selectfont Jack Dolgin \endgroup  &
\\
& Center for Cognitive Neuroscience                                       & \href{https://twitter.com/jbdolg}{@jbdolg}
\\
& LSRC Box 90999                                                          & \href{mailto:jack.dolgin@duke.edu}{jack.dolgin@duke.edu}
\\
& Duke University, Durham, NC 27708                                       & \href{<jackdolgin.org}{jackdolgin.org}
\end{longtable}
\setmainfont[Color={444444}]{Adobe Garamond Pro}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedleft\arraybackslash}p{.25\textwidth}}
\begingroup\sectitle EDUCATION\endgroup  & Duke University                                                                                   & Durham, NC
\\
& \ind B.S., Psychology, with Distinction                                                           &
\\
& \ind Thesis: \emph{Separating the Influence of Budget and Numeric Priming on Willingness to Pay}  &
\end{longtable}
\renewcommand{\arraystretch}{1.5}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedleft\arraybackslash}p{.25\textwidth}}
\begingroup\sectitle EXPERIENCE\endgroup  & Duke University                                                               & Durham, NC
\\
& \ind Lab Manager, Egner Lab                                                   & June 2018 -
\\
& \ind Research Assistant, Huettel Lab                                          & Jan. 2017 - May 2018
\\
& \ind Research Assistant, Cognitive-Behavioral Research and Treatment Program  & May 2016 - Dec. 2016
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.05\textwidth} >{\raggedright\arraybackslash}p{0.6\textwidth} >{\raggedright\arraybackslash}p{0.15\textwidth}}
\begingroup\sectitle GRANTS\endgroup  & 2019  & Interdisciplinary Behavioral Research Center Mini-Grant from Duke University (\$600)  &
\\
& 2018  & Faculty Seed Grant from Duke University, with Peter S. Whitehead and Tobias Egner (\$33,620)  &
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.05\textwidth} >{\raggedright\arraybackslash}p{0.3\textwidth} >{\raggedright\arraybackslash}p{0.45\textwidth}}
\begingroup\sectitle AWARDS\endgroup  & 2017  & \href{https://psychandneuro.duke.edu/summer-vertical-integration-program-vip}{Vertical Integration Program} \newline Duke University (\$3,500) & Selected to be funded for a summer of research at Duke University before my senior year
\\
& 2016  & \href{https://library.duke.edu/research/awards/aptman}{Lowell Aptman Prize} \newline Duke University (\$1,000)                                & Awarded the best research \href{https://hdl.handle.net/10161/16739}{paper} in any subject among all freshmen and sophomores at Duke University
\\
& 2015  & \href{https://library.duke.edu/research/awards/holsti}{Ole R. Holsti Prize} \newline Duke University (\$1,000) & Awarded, for a \href{http://hdl.handle.net/10161/10217}{separate paper}, the best research paper in political science using primary sources among all freshmen and sophomores at Duke University
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.8\textwidth}}
\begingroup\sectitle MANUSCRIPTS\endgroup  & \textbf{In Progress}                                                                                                                                                                  \\
& Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. Individual differences in the use of variable budget information in consumer choice. Submitted.                               \\
& Bejjani, C., \textbf{Dolgin, J.}, Zhang, Z., and Egner, T. (\href{https://osf.io/7jfbp/}{Preregistered Direct Replication, Stage 1 in principle acceptance at \emph{Psychological Science}}). Disentangling the roles of cue visibility and preparatory knowledge in learning cognitive control.
\\
& \textbf{Dolgin, J.}, Fiebelkorn, I.C., and Egner, T. TBD. In preparation.
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.8\textwidth}}
{\begingroup\sectitle TALKS \& PRESENTATIONS\endgroup}  & \textbf{Refereed} \begin{spacing}{1.5} \par \end{spacing} Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. (2019, June). Individual differences in the use of variable budget information in consumer choice. Talk delivered at the 9th annual Interdisciplinary Symposium on Neuroscience, Durham, N.C.
\\
& Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. (2019, May). \href{content/posters/Amasino_et_al_2019_APS.pdf}{Individual differences in the use of variable budget information in consumer choice}. Poster presented at the 31st annual meeting of the Association for Psychological Science, Washington, D.C.
\\
& \textbf{Dolgin, J.}, Bejjani, C., Zhang, Z., and Egner, T. (2019, May). \href{content/posters/Dolgin_et_al_2019_APS.pdf}{Disentangling the roles of cue visibility and knowledge in learning cognitive control}. Poster presented at the 31st annual meeting of the Association for Psychological Science, Washington, D.C.
\\
& Zhang, Z., Bejjani, C., \textbf{Dolgin, J.}, and Egner, T. (2019, Mar.). \href{content/posters/Zhang_et_al_2019_CNS.pdf}{Disentangling the roles of cue visibility and knowledge in learning cognitive control}. Poster presented at the 26th annual meeting of the Cognitive Neuroscience Society, San Francisco, CA.
\\
& Amasino, D., \textbf{Dolgin, J.}, and Huettel, S.A. (2018, Oct.). \href{content/posters/Amasino_et_al_2018_SNE.pdf}{Individual differences in the use of variable budget information in consumer choice}. Poster presented at the 16th annual meeting of the Society for NeuroEconomics, Philadelphia, PA.
\\
& \textbf{Non-Refereed}
\\
& Zhang, Z., Bejjani, C., Chiu, Y.C., \textbf{Dolgin, J.}, and Egner, T. (2019, July). Neural evidence of control state reinstatement: an fMRI study. Poster presented at the Duke Summer Undergraduate Research Showcase, Durham, NC.
\\
& \textbf{Dolgin, J.} (2019, May). NCAA women’s basketball as a proxy for gender differences in verbal aggression. Talk delivered at the Duke Center for Cognitive Neuroscience Data Blitz, Chapel Hill, NC.
\\
& \textbf{Dolgin, J.}, Amasino, D., and Huettel, S.A. (2018, Apr.). \href{content/posters/Dolgin_et_al_2018_Visible_Thinking.pdf}{Separating the influence of budget and numeric priming on willingness to pay}. Poster presented at the Duke Visible Thinking Poster Fair, Durham, NC.
\\
& \textbf{Dolgin, J.}, Amasino, D., and Huettel, S.A. (2017, July). \href{content/posters/Dolgin_et_al_2017_VIP.pdf}{Budget’s Effect on consumers’ willingness-to-pay}. Poster presented at the Duke Vertical Integration Program Poster Fair, Durham, NC.
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.35\textwidth} >{\raggedright\arraybackslash}p{0.45\textwidth}}
\begingroup\sectitle SIDE PROJECTS\endgroup  & \textbf{Dolgin, J.}, Whitehead, P.S., Vieth, A.Z. (In Preparation). NCAA women’s basketball reveals men as more verbally aggressive. & Foray into 50,000 basketball games with web-scraping—finishing a project idea I had to propose for Dr. Vieth’s class as an undergraduate—to complete the first observational study in adults to date on differences in verbal aggression between genders.
\\
& Butchireddygari, L., \textbf{Dolgin, J.} (2018). \href{https://www.dukechronicle.com/article/2018/01/is-greek-life-at-duke-as-homogenous-as-you-think}{Is Greek life at Duke as homogeneous as you think?} Duke Chronicle.                  & My coauthor and I collected over 30 variables on all 1,700 students in Duke’s class of 2018 to analyze trends in the demographics of members of Greek life, and we posted all our publicly-collected data to a repository and I analyzed it in an interactive \href{https://jackdolgin.shinyapps.io/GreekLifeDemographics/}{Shiny app}.
\\
& \textbf{Dolgin, J.} (2017). \href{https://www.dukechronicle.com/search?a&ti=history&au=Dolgin}{This Week in Duke Sports History}. Duke Chronicle.                                                        & My work documenting a zany fact in Duke sports history for an entire year was a finalist in Duke’s annual award for a journalism project or article across all of Duke’s undergraduates, including journalism certificate students and the rest of the school.
\end{longtable}
\begin{longtable}
{>{\raggedleft\arraybackslash}p{\HeadSpace} >{\raggedright\arraybackslash}p{0.8\textwidth}}
\begingroup\sectitle SKILLS\endgroup  & \textbf{Dolgin, J.}, Whitehead, P.S., Vieth, A.Z. (In Preparation). NCAA women’s basketball reveals men as more verbally aggressive.
\end{longtable}
